{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T08:49:15.587592Z",
     "start_time": "2025-12-01T08:49:15.569032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "# 从CSV文件导入数据\n",
    "with open('sentiment_labels_to_finetune.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    fine_tune_text = [(row[0], row[1]) for row in reader]\n",
    "\n",
    "# 现在fine_tune_text就包含了您的数据\n",
    "print(\"✅ 数据已成功导入，共\", len(fine_tune_text), \"条记录\")\n",
    "print(\"示例数据:\", fine_tune_text[0])"
   ],
   "id": "f91f4da5a3c4848d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 数据已成功导入，共 5586 条记录\n",
      "示例数据: ('公告：公司正常经营，无应披露未披露事项', 'none')\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T12:10:50.525444Z",
     "start_time": "2025-11-30T12:10:50.521160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_list = [\"none\", \"disgust\", \"happiness\", \"like\", \"fear\", \"sadness\", \"anger\", \"surprise\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "texts, str_labels = zip(*fine_tune_text)  # 解包元组\n",
    "labels = [label2id[label] for label in str_labels]  # 转为整数"
   ],
   "id": "5eb0388aad7272db",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T12:11:22.266959Z",
     "start_time": "2025-11-30T12:10:52.486704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 导入tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "model_path = r\"D:\\浏览器下载\\hugging_face\\xuyuan-trial-sentiment-bert-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ],
   "id": "91290c09f3ca4b49",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T12:11:48.035253Z",
     "start_time": "2025-11-30T12:11:47.012387Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# 对所有文本批量编码\n",
    "encodings = tokenize_function(list(texts))\n",
    "\n",
    "from datasets import Dataset\n",
    "dataset_dict = {\n",
    "    \"input_ids\": encodings[\"input_ids\"],\n",
    "    \"attention_mask\": encodings[\"attention_mask\"],\n",
    "    \"labels\": labels\n",
    "}\n",
    "#\n",
    "dataset = Dataset.from_dict(dataset_dict) #能够对数据集转化成Dataset的格式进行储存"
   ],
   "id": "f2d6c85a76b1049e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T12:11:52.061969Z",
     "start_time": "2025-11-30T12:11:52.046503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = dataset.train_test_split(test_size=0.15)  # 85% train, 15% eval\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n"
   ],
   "id": "fd202425169e4a35",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T12:13:55.678348Z",
     "start_time": "2025-11-30T12:13:55.471569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 导入本地部署的bert模型\n",
    "# 导入peft库，加载lora微调参数，并把参数加载到bert模型之中\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "from peft import get_peft_model,LoraConfig,TaskType\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,num_labels=8,id2label=id2label,label2id=label2id)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.15,\n",
    "    target_modules=['query','key','value']\n",
    ")\n",
    "\n",
    "model = get_peft_model(model,peft_config)\n",
    "print(model.print_trainable_parameters())\n"
   ],
   "id": "10da1a5a7c96c10b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 890,888 || all params: 103,164,688 || trainable%: 0.8636\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T12:16:36.981052Z",
     "start_time": "2025-11-30T12:16:36.925934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载训练参数\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=r'D:\\浏览器下载\\hugging_face\\finetuned_bert_lora2',\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_accuracy',\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    report_to='none',\n",
    "    fp16 = True if torch.cuda.is_available() else False,\n",
    ")"
   ],
   "id": "e9c1be4610083229",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T12:14:27.824124Z",
     "start_time": "2025-11-30T12:14:27.662363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "# 加载精确度计算函数\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = evaluate.load('accuracy')\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis = 1)\n",
    "    results = metric.compute(predictions = predictions, references = labels)\n",
    "    return results"
   ],
   "id": "9f47d39a7514e52b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T12:18:56.009459Z",
     "start_time": "2025-11-30T12:16:44.518547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 实例化Trainer，调用训练函数\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,                  # 用于动态 padding（可选）\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# 开始训练！\n",
    "trainer.train()"
   ],
   "id": "17b7896244fff0b1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mazal\\AppData\\Local\\Temp\\ipykernel_38584\\3660189456.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1485' max='1485' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1485/1485 02:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.710700</td>\n",
       "      <td>0.516387</td>\n",
       "      <td>0.815036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.528300</td>\n",
       "      <td>0.442497</td>\n",
       "      <td>0.847255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.378231</td>\n",
       "      <td>0.873508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.260500</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>0.883055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.365017</td>\n",
       "      <td>0.890215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "83c65b174394c41e89f4dc655905a15c"
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\mazal\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--accuracy\\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Nov 26 19:25:15 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from C:\\Users\\mazal\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--accuracy\\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Nov 26 19:25:15 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from C:\\Users\\mazal\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--accuracy\\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Nov 26 19:25:15 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from C:\\Users\\mazal\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--accuracy\\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Nov 26 19:25:15 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from C:\\Users\\mazal\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--accuracy\\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Nov 26 19:25:15 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1485, training_loss=0.4410545959215774, metrics={'train_runtime': 131.2004, 'train_samples_per_second': 180.945, 'train_steps_per_second': 11.319, 'total_flos': 382145523204480.0, 'train_loss': 0.4410545959215774, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T08:40:56.537811Z",
     "start_time": "2025-12-01T08:40:55.881574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 微调后bert的实际转化效果\n",
    "# 在实际微调过程中本人训练了两个版本，这里使用的是第一个微调后的版本，其转化结果更为合理，以上程序展现微调过程\n",
    "from transformers import pipeline\n",
    "fine_tuned_path = r\"D:\\浏览器下载\\hugging_face\\finetuned_bert_lora\\checkpoint-280\"\n",
    "# 加载微调后的模型\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=fine_tuned_path,\n",
    "    tokenizer=fine_tuned_path,\n",
    "    device=0  # -1 表示 CPU，0 表示 GPU\n",
    ")\n",
    "\n",
    "# 测试\n",
    "text = \"利好出尽是利空，高开就是跑路机会\"\n",
    "result = sentiment_pipe(text)\n",
    "print(result)"
   ],
   "id": "88ccbfa4a40aa153",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'anger', 'score': 0.48831290006637573}]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T08:41:02.615939Z",
     "start_time": "2025-12-01T08:41:02.391575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def map_to_financial_sentiment(label: str) -> str:\n",
    "    \"\"\"\n",
    "    将模型输出的8类情绪映射为金融场景下的情绪倾向\n",
    "    \"\"\"\n",
    "    positive_labels = {\"like\", \"happiness\",\"surprise\"}          # 看多、乐观\n",
    "    negative_labels = {\"anger\", \"disgust\", \"fear\", \"sadness\"}  # 看空、悲观\n",
    "    neutral_labels = {\"none\"}            # 中性或不确定（surprise 在股吧常为中性）\n",
    "\n",
    "    if label in positive_labels:\n",
    "        return \"Positive\"\n",
    "    elif label in negative_labels:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"中文股吧情绪分析测试（基于细粒度情感模型）\")\n",
    "print(\"=\" * 60)\n",
    "financial_texts = [\n",
    "    \"60日线下缩量滞涨，故事讲完了吗？那就要慢慢消化高市盈了，路漫漫其修远兮\",\n",
    "    \"尾盘减了加仓部分，总体是增仓的。\",\n",
    "    \"这是在等中芯北方收购结果的节奏吗？\",\n",
    "    \"兄弟们走了，留下发财\",\n",
    "    \"吊车尾\",\n",
    "    \"中芯走势软不拉几\",\n",
    "    \"全仓干起来啊。。未来一定涨到300。。回头看这里就是洼地。。补仓锁仓三年\",\n",
    "    \"你给华虹提鞋的？\",\n",
    "    \"这个股票是散户的大本营，结果有机都不来，这几天就是没有成交量\"\n",
    "]\n",
    "\n",
    "for text in financial_texts:\n",
    "    try:\n",
    "        result = sentiment_pipe(text)[0]\n",
    "        raw_label = result['label']\n",
    "        confidence = result['score']\n",
    "        financial_label = map_to_financial_sentiment(raw_label.lower())\n",
    "\n",
    "        print(f\"文本: {text}\")\n",
    "        print(f\"原始情绪: {raw_label} → 金融情绪: {financial_label} (置信度: {confidence:.2%})\")\n",
    "        print(\"-\" * 60)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 处理文本出错: {text[:30]}... | 错误: {e}\")\n",
    "        print(\"-\" * 60)"
   ],
   "id": "613268da82cadff7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "中文股吧情绪分析测试（基于细粒度情感模型）\n",
      "============================================================\n",
      "文本: 60日线下缩量滞涨，故事讲完了吗？那就要慢慢消化高市盈了，路漫漫其修远兮\n",
      "原始情绪: fear → 金融情绪: Negative (置信度: 29.45%)\n",
      "------------------------------------------------------------\n",
      "文本: 尾盘减了加仓部分，总体是增仓的。\n",
      "原始情绪: none → 金融情绪: Neutral (置信度: 98.06%)\n",
      "------------------------------------------------------------\n",
      "文本: 这是在等中芯北方收购结果的节奏吗？\n",
      "原始情绪: disgust → 金融情绪: Negative (置信度: 69.63%)\n",
      "------------------------------------------------------------\n",
      "文本: 兄弟们走了，留下发财\n",
      "原始情绪: surprise → 金融情绪: Positive (置信度: 91.83%)\n",
      "------------------------------------------------------------\n",
      "文本: 吊车尾\n",
      "原始情绪: sadness → 金融情绪: Negative (置信度: 90.23%)\n",
      "------------------------------------------------------------\n",
      "文本: 中芯走势软不拉几\n",
      "原始情绪: fear → 金融情绪: Negative (置信度: 32.20%)\n",
      "------------------------------------------------------------\n",
      "文本: 全仓干起来啊。。未来一定涨到300。。回头看这里就是洼地。。补仓锁仓三年\n",
      "原始情绪: happiness → 金融情绪: Positive (置信度: 72.11%)\n",
      "------------------------------------------------------------\n",
      "文本: 你给华虹提鞋的？\n",
      "原始情绪: anger → 金融情绪: Negative (置信度: 52.20%)\n",
      "------------------------------------------------------------\n",
      "文本: 这个股票是散户的大本营，结果有机都不来，这几天就是没有成交量\n",
      "原始情绪: disgust → 金融情绪: Negative (置信度: 47.99%)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
